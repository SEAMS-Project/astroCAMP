
# üåå **astroCAMP Framework**

<img width="682" height="425" alt="image" src="https://github.com/user-attachments/assets/20cc4fcc-3351-45a1-a5c9-6c1f0e505557" />

## üîç **What is astroCAMP?**

**astroCAMP (Astronomical Co-design Analysis and Metrics Platform)** is a community-driven framework for evaluating radio-interferometric imaging pipelines under **performance**, **energy**, **scientific quality**, and **sustainability** constraints.

Its goal is to support **hardware‚Äìsoftware co-design** for SKA-scale workloads (SKA1-Low and SKA1-Mid) by enabling reproducible, quantitative exploration of:

* system-level behaviour (runtime, energy, throughput),
* platform-level device utilisation (CPU/GPU/FPGA/ASIC),
* algorithmic scientific fidelity (RMS, PSNR, astrometry, photometry, spectra),
* carbon and cost efficiency.

astroCAMP provides **datasets, baseline implementations, standard metrics, and evaluation tools**, allowing fair comparison across heterogeneous architectures and imaging approaches.



# üéØ **Why astroCAMP?**

The Square Kilometre Array (SKA) will operate under **strict power (2‚Äì5 MW)** and **cost** envelopes while processing **petascale imaging workloads**.
Most existing imaging benchmarks:

* measure performance **only**,
* neglect scientific fidelity,
* ignore carbon and economic constraints,
* use inconsistent metrics across tools,
* lack reproducibility across HPC systems.

**astroCAMP fills this gap** by introducing a **unified, multi-layer metric suite** and a reproducible **benchmarking protocol** for co-design.



# üß© **What‚Äôs in this Repository?**

* **Standardised benchmark datasets** for SKA-like workloads
* **Reference output dirty images** for quality comparison
* **A unified suite of performance‚Äìquality‚Äìsustainability‚Äìeconomic metrics**
* **Baseline imaging pipelines** (e.g., WSClean, IDG)
* **Scripts for power, memory, throughput, and fidelity evaluation**
* **Configuration files** for running controlled experiments
* **Documentation and reproducibility protocol**



# üöÄ **Quick Start**

```bash
# Clone the repository
git clone git@github.com:SEAMS-Project/astroCAMP.git
cd astrocamp

# Run a benchmark configuration
./scripts/run_benchmark.sh configs/wsclean_ska_low.yaml

# Evaluate quality and system metrics
./scripts/evaluate_metrics.py results/wsclean_ska_low/
```

Outputs include:

* system-level logs
* energy traces
* quality metrics 
* sustainability and cost metrics
* comparison plots vs. reference images

---

# üìê **Core astroCAMP Co-Design Metrics**

astroCAMP defines **four co-design layers**, each quantifying a different aspect of imaging performance and scientific validity.
All symbols are defined **inline** so the table is fully self-contained.



## **1. System-Level Metrics (End-to-End Execution on Heterogeneous Nodes)**

| ID     | Metric             | Formula           | Unit           | Meaning & Notation                                          |
| ------ | ------------------ | ----------------- | -------------- | ----------------------------------------------------------- |
| **A1** | Time-to-solution   | `T_c`             | s              | Total job runtime. `T_c` = wall-clock time.                 |
| **A2** | Energy-to-solution | `E_c = ‚à´ P(t) dt` | J              | Total energy. `P(t)` = instantaneous power.                 |
| **A3** | Throughput         | `Œò = N / T_c`     | vis/s or img/s | Science processed per second. `N` = visibilities or images. |
| **A4** | Energy efficiency  | `Œ∑_E = N / E_c`   | vis/J          | Visibilities per joule.                                     |


## **2. Platform-Level Metrics (CPU / GPU / FPGA / ASIC Devices)**

| ID     | Metric            | Formula                  | Unit | Meaning & Notation                                |
| ------ | ----------------- | ------------------------ | ---- | ------------------------------------------------- |
| **A5** | Utilisation       | `U = t_active / t_total` | ‚Äì    | Device activity. `t_active` = active kernel time. |
| **A6** | Memory bandwidth  | `B_mem = Bytes / T_c`    | GB/s | Sustained device memory throughput.               |
| **A7** | Peak memory usage | `M_peak`                 | GB   | Maximum resident memory footprint.                |



## **3. Algorithmic Quality Metrics (Scientific Validity)**

| ID   | Metric                 | Formula                                           | Unit    | Meaning & Notation (Self-contained)                                                                                                                                               |
|------|------------------------|---------------------------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| B1   | Dirty-image RMS        | `œÉ_dirty = sqrt( (1/N) Œ£ (I_i ‚Äì ƒ™)¬≤ )`            | Jy/beam | Noise + artefacts in the dirty image. `I_i` = pixel values; `ƒ™` = mean pixel intensity; `N` = number of pixels; `œÉ_dirty` = root-mean-square deviation from the mean.             |
| B2   | PSNR / SSIM            | `PSNR = 10 log10( I_max¬≤ / MSE )`                 | dB / ‚Äì  | Fidelity vs reference image `I_ref`. `I_max` = maximum pixel value; `MSE` = mean squared error between reconstruction `ƒ®` and `I_ref`; SSIM = structural similarity between them. |
| B3   | Dynamic range          | `DR = I_max / œÉ_res`                              | ‚Äì       | Ratio of peak brightness to residual noise. `I_max` = brightest pixel in the image; `œÉ_res` = RMS of the residual image; higher `DR` = better faint-source detectability.         |
| B4   | Astrometric error      | `Œµ_astro = (1/N) Œ£ L2(x_i ‚Äì x_i_ref)`           | arcsec or px | Position error of detected sources. `x_i` = measured source positions; `x_i_ref` = reference (catalogue) positions; `Œµ_astro` = mean positional offset over `N` sources.     |
| B5   | Photometric error      | `Œµ_photo = (1/N) Œ£ L1(S_i ‚Äì S_i_ref)`             | Jy      | Flux-density error. `S_i` = measured flux densities; `S_i_ref` = reference fluxes; `Œµ_photo` = mean absolute flux difference over `N` matched sources.                            |
| B6   | Spectral fidelity      | `Œµ_spec = (1/N_ŒΩ) Œ£ L1 (I(ŒΩ) ‚Äì I_ref(ŒΩ))`          | Jy      | Per-channel spectral error. `I(ŒΩ)` = reconstructed intensity at frequency `ŒΩ`; `I_ref(ŒΩ)` = reference intensity; `N_ŒΩ` = number of frequency channels; `Œµ_spec` = mean absolute per-channel deviation. |



## **4. Sustainability Metrics (Energy ‚Üí Carbon)**

| ID     | Metric             | Formula              | Unit      | Meaning & Notation                                  |
| ------ | ------------------ | -------------------- | --------- | --------------------------------------------------- |
| **C1** | Carbon-to-solution | `C_c = E_c * Œ∫(t,r)` | gCO‚ÇÇe     | Carbon footprint. `Œ∫(t,r)` = grid carbon intensity. |
| **C2** | Carbon efficiency  | `Œ∑_C = N / C_c`      | vis/gCO‚ÇÇe | Science per gram CO‚ÇÇ emitted.                       |



## **5. Economic Metrics (Cost-Aware Co-Design)**

| ID     | Metric                  | Formula                    | Unit  | Meaning & Notation                                  |
| ------ | ----------------------- | -------------------------- | ----- | --------------------------------------------------- |
| **E1** | Total cost of ownership | `C_TTO = C_capex + C_opex` | ‚Ç¨     | Hardware lifetime cost.                             |
| **E2** | Cost per job            | `C_E = E_c * p_E`          | ‚Ç¨     | Monetary execution cost. `p_E` = electricity price. |
| **E3** | Cost efficiency         | `Œò / C_TTO`                | ops/‚Ç¨ | Science per euro invested.                          |


# üß™ **Benchmark Datasets**

AstroCAMP includes curated datasets representing:

* **SKA1-Low** visibility volumes
* [TODO] **SKA1-Mid** continuum datasets
* **Dirty-image references** for quality verification

All datasets include metadata describing:
* reference outputs,
* [TODO] numerical precision requirements,
* [TODO] acceptable tolerances


---

# üèóÔ∏è **Repository Structure**

```
astrocamp/
‚îÇ
‚îú‚îÄ‚îÄ datasets/        # Standard benchmark datasets + references
‚îú‚îÄ‚îÄ metrics/         # Metric definitions, measurement tools, analysis scripts
‚îú‚îÄ‚îÄ baselines/       # Baseline pipelines (WSClean, IDG, etc.)
‚îú‚îÄ‚îÄ tools/           # Power measurement, GPU/CPU monitoring, image stats
‚îú‚îÄ‚îÄ configs/         # YAML/JSON benchmark configs
‚îú‚îÄ‚îÄ scripts/         # Benchmark runners + evaluation utilities
‚îú‚îÄ‚îÄ results/         # Local results directory (auto-generated)
‚îî‚îÄ‚îÄ docs/            # Protocol, methodology, design notes
```

---

# üìÑ **Benchmarking Protocol (Short Summary)**

astroCAMP‚Äôs protocol ensures:

* **Reproducibility:** fixed configs, standard datasets, controlled measurement tools
* **Comparability:** consistent metrics across algorithms/tools/architectures
* **Scientific validity:** quality metrics tied to astronomical requirements
* **Co-design relevance:** integrates performance, energy, carbon, and cost

A full specification is provided in `/docs/protocol.md`.

---

# ü§ù **Contribute**

astroCAMP is a **community benchmark**.
Contributions welcome for:

* new datasets
* new imaging pipelines
* new target architectures (FPGA, ASIC, RISC-V)
* improved metrics
* documentation, tutorials, results



# üì¨ **Contact**

For questions, collaborations, or adding your pipeline to the benchmark suite, please open an issue or contact the maintainers.



